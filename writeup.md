Jessica Luna lunajess@usc.edu

My project uses the News Headline for Sarcasm Detection to classify news articles that include sarcasm in contrast to those that don't use it.

# Dataset:
The dataset included three attributes for each news headline. It included the name of the headline, the link to it, and whether it is sarcastic (denoted by a 1) or not (denoted by a 0). I didn't do any preprocessing steps to the data since it didn't really have many attributes I needed to drop or fix.

# Model Development and Training:
I first parsed through the data and then tokenized the data with the Bert Tokenizer. I went through the headlines and added the special CLS and SEP tokens to the front/back of them and kept track of the max headline's length. Afterwards I padded the headlines to all be of the same length and created attention masks to ignore the padded tokens, as well as mapping the tokens to their word IDs. I added the encoded headlines and attention masks into their respective lists and converted these lists to tensors. Afterwards, I added these into a TensorDataset and created dataloaders for the training and validation set. I used the pretrained BERT model with linear classification and AdamW for adjusting the hyperparameters. I chose 23-5 for the learning rate, 1e-8 for the epsilon, and 4 for the epochs because these were all recommended choices for BERT. Afterwards, it was time to train the model. For the training, I measured how long the current epoch took and put it in training mode. Then I unpacked the training batch using the to method and accessing the pytorch tensors with 0 (for the input ids list), 1 (for the attention masks list), or 2 (for whether it's sarcastic or not). I cleared the gradients and performed forward pass. Afterwards, I put it in an activation function and calculated the loss. Then I performed a backward pass. Lastly, I calculated the average loss and how long the epoch took and printed this information. Once the training was complete, I moved onto validating and did almost the exact same process. I added all the information regarding training set's loss, validation set's loss, validation set's accuracy, and more into a training_stats list. I also repeated this process on the test set. It was easy to alter the BERT pretrained model to work for this dataset. I followed a lot of what we did in Notebook 4 with another dataset and applied it to this dataset which improved the performance by a lot so I think this was a good model implementation choice for my task.

# Model Evaluation/Results:
For the model evalution, one of the metrics I used was plotting the training loss and validation loss as we did in Notebook 4. The plot showed that the training loss kept going down per epoch while the validation loss increased per epoch. Another one was the Matthew's Correlation Coefficient which gave me a result of 0.981 which shows that there's a strong agreement between the predictions and true results. Other metrics I used were the displot and countplot to show the distribution between the headlines that are and aren't sarcastic and there were more 0's (14985) than 1's (11724). I also made a word cloud to see the words that showed up the most in the sarcastic headlines and found the big 2 words were "new" and "man". I printed a confusion matrix as well and found that there were 14,885 true negatives, 100 false positives, 11,575 true positives, and 149 false negatives. I used the classification_report metric and found that my precision, recall, f1-score, and overall accuracy was 99% so my model performed extremely well. Lastly, I used the roc_auc_score, roc_curve, precision_recall_curve, and auc metrics and found that the AUC score was 0.99 and that the precision-recall AUC was 0.99 so my model had a good ability to distinguish between positive and negative examples and had a great balance between precision and recall.

# Discussion:
I've discussed it above, but my model architecture and training procedures fit the task well since I followed what we did in notebook 4, but altered it to fit the dataset which ended up performing even better than the model from notebook 4. In contrast to the 0.55 MCC in notebook 4, my MCC result was close to 1.0 so I think it performed extremely well. The metrics I chose offered more information and fit the binary classification task at hand appropriately. My efforts can be extended to wider implications and contribute to social good by exploring other places where bias may occur, such as in resume screenings where being a woman might make the machine disregard your resume. Some limitations are that I feel like you have to reduce the data and not take in too much data since my model took a while to run on the entire 24,000 training samples. It did really well in its accuracy and prediction with the cost of time since it took almost 20 minutes to complete the training on, so it's really up to what one is focused on (speed vs. accuracy). If I were to continue this project, I would test it out with smaller data and see how it performs. Will it run faster? Will it remain just as accurate? Another thing I would do would be to test it out on new data and see if it can just as accurately perform binary classification on this dataset.